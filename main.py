import asyncio
import os
from typing import List, Dict, Optional
import time
import chardet
from astrbot.core.utils.astrbot_path import get_astrbot_data_path
import subprocess
from aiocqhttp.exceptions import ActionFailed

from astrbot.api.event import filter, AstrMessageEvent, MessageChain, MessageEventResult
from astrbot.api.star import Context, Star, register
from astrbot.api import logger
import astrbot.api.message_components as Comp
from astrbot.api.message_components import Reply, Plain, Node, File
from astrbot.core.platform.sources.aiocqhttp.aiocqhttp_message_event import AiocqhttpMessageEvent

@register(
    "astrbot_plugin_file_checker",
    "Foolllll",
    "ç¾¤æ–‡ä»¶å¤±æ•ˆæ£€æŸ¥",
    "1.4",
    "https://github.com/Foolllll-J/astrbot_plugin_file_checker"
)
class GroupFileCheckerPlugin(Star):
    def __init__(self, context: Context, config: Optional[Dict] = None):
        super().__init__(context)
        self.config = config if config else {}
        self.group_whitelist: List[int] = self.config.get("group_whitelist", [])
        self.group_whitelist = [int(gid) for gid in self.group_whitelist]
        self.notify_on_success: bool = self.config.get("notify_on_success", True)
        self.pre_check_delay_seconds: int = self.config.get("pre_check_delay_seconds", 10)
        self.check_delay_seconds: int = self.config.get("check_delay_seconds", 300)
        self.preview_length: int = self.config.get("preview_length", 200)
        self.forward_threshold: int = self.config.get("forward_threshold", 300)
        self.enable_zip_preview: bool = self.config.get("enable_zip_preview", True)
        self.default_zip_password: str = self.config.get("default_zip_password", "")
        self.enable_repack_on_failure: bool = self.config.get("enable_repack_on_failure", False)
        self.repack_zip_password: str = self.config.get("repack_zip_password", "")
        self.enable_duplicate_check: bool = self.config.get("enable_duplicate_check", False)
        
        self.download_semaphore = asyncio.Semaphore(5)
        logger.info("æ’ä»¶ [ç¾¤æ–‡ä»¶å¤±æ•ˆæ£€æŸ¥] å·²åŠ è½½ã€‚")

    def _find_file_component(self, event: AstrMessageEvent) -> Optional[Comp.File]:
        for segment in event.get_messages():
            if isinstance(segment, Comp.File):
                return segment
        return None

    def _fix_zip_filename(self, filename: str) -> str:
        try:
            return filename.encode('cp437').decode('gbk')
        except (UnicodeEncodeError, UnicodeDecodeError):
            return filename
    
    async def _search_file_id_by_name(self, event: AstrMessageEvent, file_name: str) -> Optional[str]:
        group_id = int(event.get_group_id())
        self_id = event.get_self_id()
        
        logger.info(f"[{group_id}] å¼€å§‹æ–‡ä»¶IDæœç´¢ï¼šç›®æ ‡æ–‡ä»¶å='{file_name}'")
        
        try:
            client = event.bot
            file_list = await client.api.call_action('get_group_root_files', group_id=group_id)
            
            if not isinstance(file_list, dict) or 'files' not in file_list:
                logger.warning("get_group_root_files APIè°ƒç”¨è¿”å›äº†æ„æ–™ä¹‹å¤–çš„æ ¼å¼ã€‚")
                return None
            
            for file_info in file_list.get('files', []):
                if file_info.get('file_name') == file_name:
                    logger.info(f"[{group_id}] æˆåŠŸåŒ¹é…ï¼æ–‡ä»¶ID: {file_info.get('file_id')}")
                    return file_info.get('file_id')
            
            logger.info(f"[{group_id}] æœªæ‰¾åˆ°åŒ¹é…æ–‡ä»¶ã€‚")
            return None
        except Exception as e:
            logger.error(f"[{group_id}] é€šè¿‡æ–‡ä»¶åæœç´¢æ–‡ä»¶IDæ—¶å‡ºé”™: {e}", exc_info=True)
            return None

    async def _check_if_file_exists_by_size(self, event: AstrMessageEvent, file_name: str, file_size: int, upload_time: int) -> List[Dict]:
        group_id = int(event.get_group_id())
        
        client = event.bot
        all_files_dict = {}
        folders_to_scan = [{'folder_id': '/', 'folder_name': 'æ ¹ç›®å½•'}]
        
        while folders_to_scan:
            current_folder = folders_to_scan.pop(0)
            current_folder_id = current_folder['folder_id']
            current_folder_name = current_folder['folder_name']
            
            try:
                if current_folder_id == '/':
                    result = await client.api.call_action('get_group_root_files', group_id=group_id)
                else:
                    result = await client.api.call_action('get_group_files_by_folder', group_id=group_id, folder_id=current_folder_id, file_count=1000)

                if not isinstance(result, dict):
                    logger.warning(f"[{group_id}] APIè¿”å›äº†æ„æ–™ä¹‹å¤–çš„æ ¼å¼ã€‚")
                    continue
                
                for file_info in result.get('files', []):
                    file_info['parent_folder_name'] = current_folder_name
                    all_files_dict[file_info.get('file_id')] = file_info
                
                for folder_info in result.get('folders', []):
                    folders_to_scan.append(folder_info)

            except Exception as e:
                logger.error(f"[{group_id}] éå†æ–‡ä»¶å¤¹ '{current_folder['folder_name']}' æ—¶å‡ºé”™: {e}", exc_info=True)
        
        logger.info(f"[{group_id}] éå†å®Œæˆï¼Œå…±æ‰¾åˆ° {len(all_files_dict)} ä¸ªæ–‡ä»¶ã€‚")
        
        possible_duplicates = []
        for file_info in all_files_dict.values():
            if file_info.get('file_size') == file_size:
                possible_duplicates.append(file_info)

        logger.info(f"[{group_id}] å…±æ‰¾åˆ° {len(possible_duplicates)} ä¸ªå¤§å°åŒ¹é…çš„å€™é€‰é¡¹ã€‚")
        
        existing_files = []
        removed_files = []
        
        for f in possible_duplicates:
            file_modify_time = f.get('modify_time')
            
            if file_modify_time is not None:
                file_time_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(file_modify_time))
                
                # æ¯”è¾ƒæ—¶é—´æˆ³
                if file_modify_time == upload_time:
                    removed_files.append(f)
                else:
                    existing_files.append(f)
            else:
                existing_files.append(f)

        if removed_files:
            logger.info(f"[{group_id}] å·²ä»å€™é€‰é¡¹ä¸­æ’é™¤è‡ªèº«æ–‡ä»¶ï¼Œå…± {len(removed_files)} ä¸ªã€‚")
        
        if existing_files:
            logger.info(f"[{group_id}] æœ€ç»ˆç¡®è®¤ {len(existing_files)} ä¸ªçœŸæ­£çš„é‡å¤æ–‡ä»¶ã€‚")
            for f in existing_files:
                modify_time_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(f.get('modify_time', 0)))
                logger.info(
                    f"  â†³ æ–‡ä»¶å: '{f.get('file_name', 'æœªçŸ¥')}'\n"
                    f"    æ–‡ä»¶ID: {f.get('file_id', 'æœªçŸ¥')}\n"
                    f"    å¤§å°: {f.get('file_size', 'æœªçŸ¥')}å­—èŠ‚\n"
                    f"    ä¸Šä¼ è€…: {f.get('uploader_name', 'æœªçŸ¥')}\n"
                    f"    ä¿®æ”¹æ—¶é—´: {modify_time_str}\n"
                    f"    æ‰€å±æ–‡ä»¶å¤¹: {f.get('parent_folder_name', 'æ ¹ç›®å½•')}"
                )
        else:
            logger.info(f"[{group_id}] æœªæ‰¾åˆ°çœŸæ­£çš„é‡å¤æ–‡ä»¶ã€‚")
        
        return existing_files
    
    @filter.event_message_type(filter.EventMessageType.GROUP_MESSAGE, priority=2)
    async def on_group_message(self, event: AstrMessageEvent, *args, **kwargs):
        group_id = int(event.get_group_id())
        if self.group_whitelist and group_id not in self.group_whitelist:
            return
        
        try:
            raw_event_data = event.message_obj.raw_message
            message_list = raw_event_data.get("message")
            if not isinstance(message_list, list):
                return
            for segment_dict in message_list:
                if isinstance(segment_dict, dict) and segment_dict.get("type") == "file":
                    data_dict = segment_dict.get("data", {})
                    file_name = data_dict.get("file")
                    file_id = data_dict.get("file_id")
                    file_size = data_dict.get("file_size")

                    if isinstance(file_size, str):
                        try:
                            file_size = int(file_size)
                        except ValueError:
                            logger.error(f"æ— æ³•å°†æ–‡ä»¶å¤§å° '{file_size}' è½¬æ¢ä¸ºæ•´æ•°ï¼Œå·²è·³è¿‡é‡å¤æ€§æ£€æŸ¥ã€‚")
                            file_size = None

                    if file_name and file_id:
                        logger.info(f"ã€åŸå§‹æ–¹å¼ã€‘æˆåŠŸè§£æ: æ–‡ä»¶å='{file_name}', ID='{file_id}'")
                        file_component = self._find_file_component(event)
                        if not file_component:
                            logger.error("è‡´å‘½é”™è¯¯ï¼šæ— æ³•åœ¨é«˜çº§ç»„ä»¶ä¸­æ‰¾åˆ°å¯¹åº”çš„Fileå¯¹è±¡ï¼")
                            return
                        
                        if self.enable_duplicate_check and file_size is not None:
                            # è·å–æ–‡ä»¶ä¸Šä¼ æ—¶çš„ç²¾ç¡®æ—¶é—´æˆ³
                            upload_time = raw_event_data.get("time", int(time.time()))
                            logger.info(f"[{group_id}] æ–°ä¸Šä¼ æ–‡ä»¶æ—¶é—´æˆ³: {upload_time} ({time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(upload_time))})")

                            existing_files = await self._check_if_file_exists_by_size(event, file_name, file_size, upload_time)
                            if existing_files:
                                if len(existing_files) == 1:
                                    existing_file = existing_files[0]
                                    reply_text = (
                                        f"ğŸ’¡ æé†’ï¼šæ‚¨å‘é€çš„æ–‡ä»¶ã€Œ{file_name}ã€å¯èƒ½ä¸ç¾¤æ–‡ä»¶ä¸­çš„ã€Œ{existing_file.get('file_name', 'æœªçŸ¥æ–‡ä»¶å')}ã€é‡å¤ã€‚\n"
                                        f"  â†³ ä¸Šä¼ è€…: {existing_file.get('uploader_name', 'æœªçŸ¥')}\n"
                                        f"  â†³ ä¿®æ”¹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(existing_file.get('modify_time', 0)))}\n"
                                        f"  â†³ æ‰€å±æ–‡ä»¶å¤¹: {existing_file.get('parent_folder_name', 'æ ¹ç›®å½•')}"
                                    )
                                    await self._send_or_forward(event, reply_text, event.message_obj.message_id)
                                else:
                                    reply_text = f"ğŸ’¡ æé†’ï¼šæ‚¨å‘é€çš„æ–‡ä»¶ã€Œ{file_name}ã€å¯èƒ½ä¸ç¾¤æ–‡ä»¶ä¸­ä»¥ä¸‹ {len(existing_files)} ä¸ªæ–‡ä»¶é‡å¤ï¼š\n"
                                    for idx, file_info in enumerate(existing_files, 1):
                                        reply_text += (
                                            f"\n{idx}. {file_info.get('file_name', 'æœªçŸ¥æ–‡ä»¶å')}\n"
                                            f"    â†³ ä¸Šä¼ è€…: {file_info.get('uploader_name', 'æœªçŸ¥')}\n"
                                            f"    â†³ ä¿®æ”¹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(file_info.get('modify_time', 0)))}\n"
                                            f"    â†³ æ‰€å±æ–‡ä»¶å¤¹: {file_info.get('parent_folder_name', 'æ ¹ç›®å½•')}"
                                        )
                                    await self._send_or_forward(event, reply_text, event.message_obj.message_id)
                                return

                        await self._handle_file_check_flow(event, file_name, file_id, file_component)
                        break
        except Exception as e:
            logger.error(f"ã€åŸå§‹æ–¹å¼ã€‘å¤„ç†æ¶ˆæ¯æ—¶å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}", exc_info=True)

    async def _send_or_forward(self, event: AstrMessageEvent, text: str, message_id: int):
        if self.forward_threshold <= 0 or len(text) <= self.forward_threshold:
            chain = MessageChain([Reply(id=message_id), Plain(text=text)])
            await event.send(chain)
            return
        logger.info(f"[{event.get_group_id()}] æ£€æµ‹åˆ°é•¿æ¶ˆæ¯ï¼Œå°†è‡ªåŠ¨åˆå¹¶è½¬å‘ã€‚")
        try:
            forward_node = Node(uin=event.get_self_id(), name="æ–‡ä»¶æ£€æŸ¥æŠ¥å‘Š", content=[Reply(id=message_id), Plain(text=text)])
            await event.send(MessageChain([forward_node]))
        except Exception as e:
            logger.error(f"[{event.get_group_id()}] åˆå¹¶è½¬å‘é•¿æ¶ˆæ¯æ—¶å‡ºé”™: {e}", exc_info=True)
            fallback_text = text[:self.forward_threshold] + "... (æ¶ˆæ¯è¿‡é•¿ä¸”åˆå¹¶è½¬å‘å¤±è´¥)"
            chain = MessageChain([Reply(id=message_id), Plain(text=fallback_text)])
            await event.send(chain)

    async def _repack_and_send_txt(self, event: AstrMessageEvent, original_filename: str, file_component: Comp.File):
        temp_dir = os.path.join(get_astrbot_data_path(), "plugins_data", "file_checker", "temp")
        os.makedirs(temp_dir, exist_ok=True)
        
        repacked_file_path = None
        original_txt_path = None
        renamed_txt_path = None
        try:
            logger.info(f"å¼€å§‹ä¸ºå¤±æ•ˆæ–‡ä»¶ {original_filename} è¿›è¡Œé‡æ–°æ‰“åŒ…...")
            
            original_txt_path = await file_component.get_file()
            
            renamed_txt_path = os.path.join(temp_dir, original_filename)
            if os.path.exists(renamed_txt_path):
                os.remove(renamed_txt_path)
            os.rename(original_txt_path, renamed_txt_path)

            base_name = os.path.splitext(original_filename)[0]
            new_zip_name = f"{base_name}.zip"
            repacked_file_path = os.path.join(temp_dir, f"{int(time.time())}_{new_zip_name}")

            command = ['zip', '-j', repacked_file_path, renamed_txt_path]
            if self.repack_zip_password:
                command.extend(['-P', self.repack_zip_password])

            logger.info(f"æ­£åœ¨æ‰§è¡Œæ‰“åŒ…å‘½ä»¤: {' '.join(command)}")
            process = await asyncio.create_subprocess_exec(
                *command,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )
            stdout, stderr = await process.communicate()
            
            if process.returncode != 0:
                error_message = stderr.decode('utf-8')
                logger.error(f"ä½¿ç”¨ zip å‘½ä»¤æ‰“åŒ…æ–‡ä»¶æ—¶å‡ºé”™: {error_message}")
                await event.send(MessageChain([Plain(f"âŒ é‡æ–°æ‰“åŒ…å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯ï¼š\n{error_message}")]))
                return
            
            logger.info(f"æ–‡ä»¶å·²é‡æ–°æ‰“åŒ…è‡³ {repacked_file_path}ï¼Œå‡†å¤‡å‘é€...")
            
            reply_text = "å·²ä¸ºæ‚¨é‡æ–°æ‰“åŒ…ä¸ºZIPæ–‡ä»¶å‘é€ï¼š"
            file_component_to_send = File(file=repacked_file_path, name=new_zip_name)
            
            await event.send(MessageChain([Plain(reply_text)]))
            
            await event.send(MessageChain([file_component_to_send]))
            
            await asyncio.sleep(2)
            
            new_file_id = await self._search_file_id_by_name(event, new_zip_name)
            
            if new_file_id:
                logger.info(f"æ–°æ–‡ä»¶å‘é€æˆåŠŸï¼ŒIDä¸º {new_file_id}ï¼Œå·²åŠ å…¥å»¶æ—¶å¤æ ¸é˜Ÿåˆ—ã€‚")
                asyncio.create_task(self._task_delayed_recheck(event, new_zip_name, new_file_id, file_component, None))
            else:
                logger.error("æœªèƒ½è·å–æ–°æ–‡ä»¶çš„IDï¼Œæ— æ³•è¿›è¡Œå»¶æ—¶å¤æ ¸ã€‚")
            
        except FileNotFoundError:
            logger.error("é‡æ–°æ‰“åŒ…å¤±è´¥ï¼šå®¹å™¨å†…æœªæ‰¾åˆ° zip å‘½ä»¤ã€‚è¯·å®‰è£… zipã€‚")
            await event.send(MessageChain([Plain("âŒ é‡æ–°æ‰“åŒ…å¤±è´¥ã€‚å®¹å™¨å†…æœªæ‰¾åˆ° zip å‘½ä»¤ï¼Œè¯·è”ç³»ç®¡ç†å‘˜å®‰è£…ã€‚")]))
        except Exception as e:
            logger.error(f"é‡æ–°æ‰“åŒ…å¹¶å‘é€æ–‡ä»¶æ—¶å‡ºé”™: {e}", exc_info=True)
            await event.send(MessageChain([Plain("âŒ é‡æ–°æ‰“åŒ…å¹¶å‘é€æ–‡ä»¶å¤±è´¥ã€‚")]))
        finally:
            if repacked_file_path and os.path.exists(repacked_file_path):
                async def cleanup_file(path: str):
                    await asyncio.sleep(10)
                    try:
                        os.remove(path)
                        logger.info(f"å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {path}")
                    except OSError as e:
                        logger.warning(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶ {path} å¤±è´¥: {e}")
                asyncio.create_task(cleanup_file(repacked_file_path))

            if renamed_txt_path and os.path.exists(renamed_txt_path):
                try:
                    os.remove(renamed_txt_path)
                    logger.info(f"å·²æ¸…ç†é‡å‘½ååçš„ä¸´æ—¶æ–‡ä»¶: {renamed_txt_path}")
                except OSError as e:
                    logger.warning(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶ {renamed_txt_path} å¤±è´¥: {e}")
    
    async def _handle_file_check_flow(self, event: AstrMessageEvent, file_name: str, file_id: str, file_component: Comp.File):
        group_id = int(event.get_group_id())
        message_id = event.message_obj.message_id
        
        sender_id = event.get_sender_id()
        self_id = event.get_self_id()
        if sender_id == self_id:
            logger.info(f"[{group_id}] æœºå™¨äººå‘é€çš„æ–‡ä»¶ï¼Œç›´æ¥è·³è¿‡å¤„ç†ã€‚")
            return
        
        await asyncio.sleep(self.pre_check_delay_seconds)
        logger.info(f"[{group_id}] [é˜¶æ®µä¸€] å¼€å§‹å³æ—¶æ£€æŸ¥: '{file_name}'")

        is_gfs_valid = await self._check_validity_via_gfs(event, file_id)

        preview_text, preview_extra_info = await self._get_preview_for_file(file_name, file_component)

        if is_gfs_valid:
            if self.notify_on_success:
                success_message = f"âœ… æ‚¨å‘é€çš„æ–‡ä»¶ã€Œ{file_name}ã€åˆæ­¥æ£€æŸ¥æœ‰æ•ˆã€‚"
                if preview_text:
                    preview_text_short = preview_text[:self.preview_length]
                    success_message += f"\n{preview_extra_info}ï¼Œä»¥ä¸‹æ˜¯é¢„è§ˆï¼š\n{preview_text_short}"
                    if len(preview_text) > self.preview_length: success_message += "..."
                await self._send_or_forward(event, success_message, message_id)
            logger.info(f"[{group_id}] åˆæ­¥æ£€æŸ¥é€šè¿‡ï¼Œå·²åŠ å…¥å»¶æ—¶å¤æ ¸é˜Ÿåˆ—ã€‚")
            asyncio.create_task(self._task_delayed_recheck(event, file_name, file_id, file_component, preview_text))
        else:
            logger.error(f"âŒ [{group_id}] [é˜¶æ®µä¸€] æ–‡ä»¶ '{file_name}' å³æ—¶æ£€æŸ¥å·²å¤±æ•ˆ!")
            try:
                failure_message = f"âš ï¸ æ‚¨å‘é€çš„æ–‡ä»¶ã€Œ{file_name}ã€å·²å¤±æ•ˆã€‚"
                if preview_text:
                    preview_text_short = preview_text[:self.preview_length]
                    failure_message += f"\n{preview_extra_info}ï¼Œä»¥ä¸‹æ˜¯é¢„è§ˆï¼š\n{preview_text_short}"
                    if len(preview_text) > self.preview_length: failure_message += "..."
                await self._send_or_forward(event, failure_message, message_id)

                is_txt = file_name.lower().endswith('.txt')
                if self.enable_repack_on_failure and is_txt and preview_text:
                    logger.info("æ–‡ä»¶å³æ—¶æ£€æŸ¥å¤±æ•ˆä½†å†…å®¹å¯è¯»ï¼Œè§¦å‘é‡æ–°æ‰“åŒ…ä»»åŠ¡...")
                    await self._repack_and_send_txt(event, file_name, file_component)
                
            except Exception as send_e:
                logger.error(f"[{group_id}] [é˜¶æ®µä¸€] å›å¤å¤±æ•ˆé€šçŸ¥æ—¶å†æ¬¡å‘ç”Ÿé”™è¯¯: {send_e}")

    async def _check_validity_via_gfs(self, event: AstrMessageEvent, file_id: str) -> bool:
        group_id = int(event.get_group_id())
        try:
            assert isinstance(event, AiocqhttpMessageEvent)
            client = event.bot
            url_result = await client.api.call_action('get_group_file_url', group_id=group_id, file_id=file_id)
            return bool(url_result and url_result.get('url'))
        except Exception:
            return False

    def _get_preview_from_bytes(self, content_bytes: bytes) -> tuple[str, str]:
        try:
            detection = chardet.detect(content_bytes)
            encoding = detection.get('encoding', 'utf-8') or 'utf-8'
            
            # ä¼˜å…ˆä½¿ç”¨ chardet çš„é«˜ç½®ä¿¡åº¦ç»“æœ
            if encoding and detection['confidence'] > 0.7:
                decoded_text = content_bytes.decode(encoding, errors='ignore').strip()
                return decoded_text, encoding
            
            # å¦‚æœç½®ä¿¡åº¦ä½æˆ–æ— æ³•è¯†åˆ«ï¼Œä½¿ç”¨ chardet çŒœæµ‹çš„æœ€å¯èƒ½ç¼–ç è¿›è¡Œå›é€€
            if encoding:
                decoded_text = content_bytes.decode(encoding, errors='ignore').strip()
                return decoded_text, f"{encoding} (ä½ç½®ä¿¡åº¦å›é€€)"
            
            return "", "æœªçŸ¥"
            
        except Exception:
            return "", "æœªçŸ¥"
            
    async def _get_preview_from_zip(self, file_path: str) -> tuple[str, str]:
        temp_dir = os.path.join(get_astrbot_data_path(), "plugins_data", "file_checker", "temp")
        os.makedirs(temp_dir, exist_ok=True)
        
        extract_path = os.path.join(temp_dir, f"extract_{int(time.time())}")
        os.makedirs(extract_path, exist_ok=True)
        
        extracted_txt_path = None
        
        try:
            # ç¬¬ä¸€æ¬¡å°è¯•ï¼šæ— å¯†ç è§£å‹
            logger.info("æ­£åœ¨å°è¯•æ— å¯†ç è§£å‹...")
            command_no_pwd = ["7za", "x", file_path, f"-o{extract_path}", "-y"]
            process = await asyncio.create_subprocess_exec(
                *command_no_pwd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )
            stdout, stderr = await process.communicate()

            if process.returncode != 0:
                # ç¬¬ä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰é»˜è®¤å¯†ç 
                if self.default_zip_password:
                    logger.info("æ— å¯†ç è§£å‹å¤±è´¥ï¼Œæ­£åœ¨å°è¯•ä½¿ç”¨é»˜è®¤å¯†ç ...")
                    # ç¬¬äºŒæ¬¡å°è¯•ï¼šä½¿ç”¨é»˜è®¤å¯†ç è§£å‹
                    command_with_pwd = ["7za", "x", file_path, f"-o{extract_path}", f"-p{self.default_zip_password}", "-y"]
                    process = await asyncio.create_subprocess_exec(
                        *command_with_pwd,
                        stdout=subprocess.PIPE,
                        stderr=subprocess.PIPE
                    )
                    stdout, stderr = await process.communicate()
                    
                    if process.returncode != 0:
                        error_message = stderr.decode('utf-8')
                        logger.error(f"ä½¿ç”¨é»˜è®¤å¯†ç è§£å‹å¤±è´¥: {error_message}")
                        return "", "è§£å‹å¤±è´¥"
                else:
                    error_message = stderr.decode('utf-8')
                    logger.error(f"ä½¿ç”¨ 7za å‘½ä»¤è§£å‹å¤±è´¥ä¸”æœªè®¾ç½®é»˜è®¤å¯†ç : {error_message}")
                    return "", "è§£å‹å¤±è´¥"

            # æˆåŠŸè§£å‹åï¼ŒæŸ¥æ‰¾ .txt æ–‡ä»¶
            all_extracted_files = os.listdir(extract_path)
            txt_files = [f for f in all_extracted_files if f.lower().endswith('.txt')]
            
            if not txt_files:
                return "", "æ— æ³•æ‰¾åˆ° .txt æ–‡ä»¶"
                
            first_txt_file = txt_files[0]
            extracted_txt_path = os.path.join(extract_path, first_txt_file)
            
            with open(extracted_txt_path, 'rb') as f:
                content_bytes = f.read(2048)
            
            preview_text, encoding = self._get_preview_from_bytes(content_bytes)
            extra_info = f"å·²è§£å‹ã€Œ{first_txt_file}ã€(æ ¼å¼ {encoding})"
            return preview_text, extra_info
            
        except FileNotFoundError:
            logger.error("è§£å‹å¤±è´¥ï¼šå®¹å™¨å†…æœªæ‰¾åˆ° 7za å‘½ä»¤ã€‚è¯·å®‰è£… p7zip-fullã€‚")
            return "", "æœªå®‰è£… 7za"
        except Exception as e:
            logger.error(f"å¤„ç†ZIPæ–‡ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
            return "", "æœªçŸ¥é”™è¯¯"
        finally:
            if extract_path and os.path.exists(extract_path):
                async def cleanup_folder(path: str):
                    await asyncio.sleep(5)
                    try:
                        for item in os.listdir(path):
                            item_path = os.path.join(path, item)
                            if os.path.isfile(item_path):
                                os.remove(item_path)
                        os.rmdir(path)
                        logger.info(f"å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤¹: {path}")
                    except OSError as e:
                        logger.warning(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤¹ {path} å¤±è´¥: {e}")
                
                asyncio.create_task(cleanup_folder(extract_path))

    async def _get_preview_for_file(self, file_name: str, file_component: Comp.File) -> tuple[str, str]:
        is_txt = file_name.lower().endswith('.txt')
        is_zip = self.enable_zip_preview and file_name.lower().endswith('.zip')
        local_file_path = None
        try:
            if not (is_txt or is_zip):
                return "", ""
            async with self.download_semaphore:
                local_file_path = await file_component.get_file()
            if is_txt:
                with open(local_file_path, 'rb') as f:
                    content_bytes = f.read(2048)
                preview_text, encoding = self._get_preview_from_bytes(content_bytes)
                extra_info = f"æ ¼å¼ä¸º {encoding}"
                return preview_text, extra_info
            if is_zip:
                return await self._get_preview_from_zip(local_file_path)
        except Exception as e:
            logger.error(f"è·å–é¢„è§ˆæ—¶ä¸‹è½½æˆ–è¯»å–æ–‡ä»¶å¤±è´¥: {e}", exc_info=True)
            return "", ""
        finally:
            if local_file_path and os.path.exists(local_file_path):
                try:
                    os.remove(local_file_path)
                except OSError as e:
                    logger.warning(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶ {local_file_path} å¤±è´¥: {e}")
        return "", ""

    async def _task_delayed_recheck(self, event: AstrMessageEvent, file_name: str, file_id: str, file_component: Comp.File, preview_text: str):
        await asyncio.sleep(self.check_delay_seconds)
        group_id = int(event.get_group_id())
        message_id = event.message_obj.message_id
        logger.info(f"[{group_id}] [é˜¶æ®µäºŒ] å¼€å§‹å»¶æ—¶å¤æ ¸: '{file_name}'")
        is_still_valid = await self._check_validity_via_gfs(event, file_id)
        if not is_still_valid:
            logger.error(f"âŒ [{group_id}] [é˜¶æ®µäºŒ] æ–‡ä»¶ '{file_name}' åœ¨å»¶æ—¶å¤æ ¸æ—¶ç¡®è®¤å·²å¤±æ•ˆ!")
            try:
                failure_message = f"âŒ ç» {self.check_delay_seconds} ç§’åå¤æ ¸ï¼Œæ‚¨å‘é€çš„æ–‡ä»¶ã€Œ{file_name}ã€å·²å¤±æ•ˆã€‚"
                await self._send_or_forward(event, failure_message, message_id)
                
                is_txt = file_name.lower().endswith('.txt')
                if self.enable_repack_on_failure and is_txt and preview_text:
                    logger.info("æ–‡ä»¶åœ¨å»¶æ—¶å¤æ ¸æ—¶å¤±æ•ˆä½†å†…å®¹å¯è¯»ï¼Œè§¦å‘é‡æ–°æ‰“åŒ…ä»»åŠ¡...")
                    await self._repack_and_send_txt(event, file_name, file_component)

            except Exception as send_e:
                logger.error(f"[{group_id}] [é˜¶æ®µäºŒ] å›å¤å¤±æ•ˆé€šçŸ¥æ—¶å†æ¬¡å‘ç”Ÿé”™è¯¯: {send_e}")
        else:
            logger.info(f"âœ… [{group_id}] [é˜¶æ®µäºŒ] æ–‡ä»¶ '{file_name}' å»¶æ—¶å¤æ ¸é€šè¿‡ï¼Œä¿æŒæ²‰é»˜ã€‚")

    async def terminate(self):
        logger.info("æ’ä»¶ [ç¾¤æ–‡ä»¶å¤±æ•ˆæ£€æŸ¥] å·²å¸è½½ã€‚")